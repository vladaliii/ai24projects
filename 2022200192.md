# Laws_rank

## 介绍

该项目实现的是可拓展文本的，通过为案例寻找关联法条的智慧法庭系统，本质是对一个预训练的bert微调完成下游任务的仓库，也是我接触人工智能后亲自实现的第一个项目，取得了较好的效果，虽然技术难度也许没有特别高，但也是相当有成就感的。
该项目的训练与评估需要通过命令行启动，我准备了可供修改的sh脚本，只需要更改bash脚本即可完成不同的训练评估任务。
为了保证评分时检测模型训练过程的纯粹性，我删除了在调试时留下的训练完成的历史模型记录，确保不出现检查点的冲突等一系列问题，评分者可以从头观测训练进程并检查结果。

另外，由于在开发起始阶段经验不足，将模型移至Ubuntu上进行了开发，一系列测试也是在Ubuntu上进行的，没有测试过对于其他操作系统的适配程度，推荐在Ubuntu下使用。

## 准备

```sh
pip install -r requirements.txt
```
请注意，transformers 的版本不宜过高，在requirement中有明确指定，存在一系列版本依赖，否则会影响训练可视化，甚至无法训练等各种问题。训练时最好保证环境内有配置好的可以使用cuda的torch，否则训练的时间代价非常难以接受。

## 使用

### 训练模型

调整 run_sts.sh 脚本中的参数，包括 model、model_type、objective 等，

然后通过运行以下命令执行脚本：

```sh
bash run_sts.sh
```
在预训练模型的选择上（也就是model参数），在最初的基准项目里model可以适配各类bert，但考虑使用 hfl/robert-chinese-wwt 及其large版本，这个中科大的模型在中文测试数据集上表现良好，在测试里也取得了比较好的效果，在国内链接huggingface有困难，该项目附带了我在调试的时候，使用的该预训练模型的基准版本（太大了，实在无法上传）。

在选择 model_type 时，有三个选项：v0、v1 和 v2。它们分别对应 LawsEncoder、LawsEncoderUsingEmbedding 和 LawsEncoderUsingBiEncoder 模型。以下是每个版本的简要概述：

v0 直接对案件和法律文本进行文本编码以获得表示。
v1 将所有法律条文映射到 ID（使用自编写的 article_tokenizer），并在训练期间通过nn.Embedding 检索相应的表示。
v2 是 v0 和 v1 的混合，结合了之前两个版本的功能。

v0 版本的模型训练速度较慢，准确率较低，但可以扩展到其他领域，可以重新输入新的法条进行拓展。v1 版本训练更快且更准确，因为本质上做的是分类任务，但无法扩展（因为 ID 在训练前已确定，更改需要完整重新训练）。v2 版本结合了两者的优点：在训练期间使用额外的损失函数来对齐 law_encoder（来自 v0）的输出与 law_embedding（来自 v1）的输出，实现了高准确率和可扩展性，本质是在forward定义的时候嵌入对齐了主编码器和法律编码器的输出，扩展时不需要重新训练。

在选择 objective 时，可以在 contrast 或 classification 之间选择。基本区别在于对比学习是在单个批次内进行分类（使用同一批次中的其他样本作为负例），而分类是在整个数据集上进行的。v0 模型仅支持对比学习，而 v1 和 v2 都支持这两种目标。通常，v1 在这两种目标下表现相似，但 v2 需要使用分类才能达到良好性能。

### 模型评估

测试模型的时候需修改 inference.py 文件开头的 model、model_type 和 objective 参数，使其与训练的模型参数匹配

我在inference.py准备了一条未被收录的案例进行评测的接口，可以通过运行以下命令执行评测脚本，v0表现较差，v1,v2表现较好：

```
python inference.py
```
在 inference.py 脚本中，有两个函数分别名为 inference_v0 和 inference_v1，需要根据训练的模型对齐这些函数，也就是手动选择（注意 v2 支持两者，也就是都能使用）。此外，可以利用 law_encoder_using_embedding_init 函数输入训练的模型、tokenizer 和 article-tokenizer，以使用一组新的法律条文进行初始化。这允许将 v0 和 v2 模型扩展到新领域进行测试，但在本例中搜集的法条已经较多，就没有再寻找确保不在训练数据内的其他文本进行拓展。

本项目在非输出区留下了一份训练完成的v1模型，可用于测试interence功能的正常使用，但因为处于非默认路径与非默认名称，测试时需要另外更改部分代码才能正确调用。
(太大了，实在无法上传)
